{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimizer",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1lIwbpfhNbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimizer.py\n",
        "class InsightFaceOptimizer(object):\n",
        "    \"\"\"A simple wrapper class for learning rate scheduling\"\"\"\n",
        "\n",
        "    def __init__(self, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.step_num = 0\n",
        "        self.lr = 0.1\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    def step(self):\n",
        "        self._update_lr()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def _update_lr(self):\n",
        "        self.step_num += 1\n",
        "        # divide the learning rate at 100K,160K iterations\n",
        "        if self.step_num in [100000, 160000]:\n",
        "            self.lr = self.lr / 10\n",
        "            for param_group in self.optimizer.param_groups:\n",
        "                param_group['lr'] = self.lr\n",
        "\n",
        "    def clip_gradient(self, grad_clip):\n",
        "        for group in self.optimizer.param_groups:\n",
        "            for param in group['params']:\n",
        "                if param.grad is not None:\n",
        "                    param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "\n",
        "    def adjust_learning_rate(self, new_lr):\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = new_lr\n",
        "        print(\"The new learning rate is %f\\n\" % (self.optimizer.param_groups[0]['lr'],))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}