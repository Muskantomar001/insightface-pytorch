{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lfw_eval",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA2A8IOaYnqp",
        "colab_type": "code",
        "outputId": "42d7f779-dbb1-4a3b-a59e-9cf044aaf9da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "#lfw_eval.py\n",
        "\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import tarfile\n",
        "import time\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "import torch\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from config import device\n",
        "from data_gen import data_transforms\n",
        "from utils import align_face, get_central_face_attributes, get_all_face_attributes, draw_bboxes\n",
        "\n",
        "angles_file = 'data/angles.txt'\n",
        "lfw_pickle = 'data/lfw_funneled.pkl'\n",
        "\n",
        "\n",
        "def extract('/content/lfw-dataset.zip'):\n",
        "    with tarfile.open('/content/lfw-dataset.zip', 'r') as tar:\n",
        "        tar.extractall('data')\n",
        "\n",
        "\n",
        "def process():\n",
        "    subjects = [d for d in os.listdir('/content/lfw-dataset.zip') if os.path.isdir(os.path.join('/content/lfw-dataset.zip', d))]\n",
        "    assert (len(subjects) == 5749), \"Number of subjects is: {}!\".format(len(subjects))\n",
        "\n",
        "    file_names = []\n",
        "    for i in tqdm(range(len(subjects))):\n",
        "        sub = subjects[i]\n",
        "        folder = os.path.join('/content/lfw-dataset.zip', sub)\n",
        "        files = [f for f in os.listdir(folder) if\n",
        "                 os.path.isfile(os.path.join(folder, f)) and f.lower().endswith('.jpg')]\n",
        "        for file in files:\n",
        "            filename = os.path.join(folder, file)\n",
        "            file_names.append({'filename': filename, 'class_id': i, 'subject': sub})\n",
        "\n",
        "    assert (len(file_names) == 13233), \"Number of files is: {}!\".format(len(file_names))\n",
        "\n",
        "    samples = []\n",
        "    for item in tqdm(file_names):\n",
        "        filename = item['filename']\n",
        "        class_id = item['class_id']\n",
        "        sub = item['subject']\n",
        "\n",
        "        try:\n",
        "            bboxes, landmarks = get_central_face_attributes(filename)\n",
        "\n",
        "            samples.append(\n",
        "                {'class_id': class_id, 'subject': sub, 'full_path': filename, 'bounding_boxes': bboxes,\n",
        "                 'landmarks': landmarks})\n",
        "        except KeyboardInterrupt:\n",
        "            raise\n",
        "        except Exception as err:\n",
        "            print(err)\n",
        "\n",
        "    with open(lfw_pickle, 'wb') as file:\n",
        "        save = {\n",
        "            'samples': samples\n",
        "        }\n",
        "        pickle.dump(save, file, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def get_image(samples, transformer, file):\n",
        "    filtered = [sample for sample in samples if file in sample['full_path'].replace('\\\\', '/')]\n",
        "    assert (len(filtered) == 1), 'len(filtered): {} file:{}'.format(len(filtered), file)\n",
        "    sample = filtered[0]\n",
        "    full_path = sample['full_path']\n",
        "    landmarks = sample['landmarks']\n",
        "    img = align_face(full_path, landmarks)  # BGR\n",
        "    # img = blur_and_grayscale(img)\n",
        "    img = img[..., ::-1]  # RGB\n",
        "    img = Image.fromarray(img, 'RGB')  # RGB\n",
        "    img = transformer(img)\n",
        "    img = img.to(device)\n",
        "    return img\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "\n",
        "    with open(lfw_pickle, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "\n",
        "    samples = data['samples']\n",
        "\n",
        "    filename = 'data/lfw_test_pair.txt'\n",
        "    with open(filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    transformer = data_transforms['val']\n",
        "\n",
        "    angles = []\n",
        "\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for line in tqdm(lines):\n",
        "            tokens = line.split()\n",
        "            file0 = tokens[0]\n",
        "            img0 = get_image(samples, transformer, file0)\n",
        "            file1 = tokens[1]\n",
        "            img1 = get_image(samples, transformer, file1)\n",
        "            imgs = torch.zeros([2, 3, 112, 112], dtype=torch.float, device=device)\n",
        "            imgs[0] = img0\n",
        "            imgs[1] = img1\n",
        "\n",
        "            output = model(imgs)\n",
        "\n",
        "            feature0 = output[0].cpu().numpy()\n",
        "            feature1 = output[1].cpu().numpy()\n",
        "            x0 = feature0 / np.linalg.norm(feature0)\n",
        "            x1 = feature1 / np.linalg.norm(feature1)\n",
        "            cosine = np.dot(x0, x1)\n",
        "            cosine = np.clip(cosine, -1.0, 1.0)\n",
        "            theta = math.acos(cosine)\n",
        "            theta = theta * 180 / math.pi\n",
        "            is_same = tokens[2]\n",
        "            angles.append('{} {}\\n'.format(theta, is_same))\n",
        "\n",
        "    elapsed_time = time.time() - start\n",
        "    print('elapsed time(sec) per image: {}'.format(elapsed_time / (6000 * 2)))\n",
        "\n",
        "    with open('data/angles.txt', 'w') as file:\n",
        "        file.writelines(angles)\n",
        "\n",
        "\n",
        "def visualize(threshold):\n",
        "    with open(angles_file) as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    ones = []\n",
        "    zeros = []\n",
        "\n",
        "    for line in lines:\n",
        "        tokens = line.split()\n",
        "        angle = float(tokens[0])\n",
        "        type = int(tokens[1])\n",
        "        if type == 1:\n",
        "            ones.append(angle)\n",
        "        else:\n",
        "            zeros.append(angle)\n",
        "\n",
        "    bins = np.linspace(0, 180, 181)\n",
        "\n",
        "    plt.hist(zeros, bins, density=True, alpha=0.5, label='0', facecolor='red')\n",
        "    plt.hist(ones, bins, density=True, alpha=0.5, label='1', facecolor='blue')\n",
        "\n",
        "    mu_0 = np.mean(zeros)\n",
        "    sigma_0 = np.std(zeros)\n",
        "    y_0 = scipy.stats.norm.pdf(bins, mu_0, sigma_0)\n",
        "    plt.plot(bins, y_0, 'r--')\n",
        "    mu_1 = np.mean(ones)\n",
        "    sigma_1 = np.std(ones)\n",
        "    y_1 = scipy.stats.norm.pdf(bins, mu_1, sigma_1)\n",
        "    plt.plot(bins, y_1, 'b--')\n",
        "    plt.xlabel('theta')\n",
        "    plt.ylabel('theta j Distribution')\n",
        "    plt.title(\n",
        "        r'Histogram : mu_0={:.4f},sigma_0={:.4f}, mu_1={:.4f},sigma_1={:.4f}'.format(mu_0, sigma_0, mu_1, sigma_1))\n",
        "\n",
        "    print('threshold: ' + str(threshold))\n",
        "    print('mu_0: ' + str(mu_0))\n",
        "    print('sigma_0: ' + str(sigma_0))\n",
        "    print('mu_1: ' + str(mu_1))\n",
        "    print('sigma_1: ' + str(sigma_1))\n",
        "\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.plot([threshold, threshold], [0, 0.05], 'k-', lw=2)\n",
        "    plt.savefig('images/theta_dist.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def accuracy(threshold):\n",
        "    with open(angles_file) as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    wrong = 0\n",
        "    for line in lines:\n",
        "        tokens = line.split()\n",
        "        angle = float(tokens[0])\n",
        "        type = int(tokens[1])\n",
        "        if type == 1:\n",
        "            if angle > threshold:\n",
        "                wrong += 1\n",
        "        else:\n",
        "            if angle <= threshold:\n",
        "                wrong += 1\n",
        "\n",
        "    accuracy = 1 - wrong / 6000\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def show_bboxes(folder):\n",
        "    with open(lfw_pickle, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "\n",
        "    samples = data['samples']\n",
        "    for sample in tqdm(samples):\n",
        "        full_path = sample['full_path']\n",
        "        bounding_boxes = sample['bounding_boxes']\n",
        "        landmarks = sample['landmarks']\n",
        "        img = cv.imread(full_path)\n",
        "        img = draw_bboxes(img, bounding_boxes, landmarks)\n",
        "        filename = os.path.basename(full_path)\n",
        "        filename = os.path.join(folder, filename)\n",
        "        cv.imwrite(filename, img)\n",
        "\n",
        "\n",
        "def error_analysis(threshold):\n",
        "    with open(angles_file) as file:\n",
        "        angle_lines = file.readlines()\n",
        "\n",
        "    fp = []\n",
        "    fn = []\n",
        "    for i, line in enumerate(angle_lines):\n",
        "        tokens = line.split()\n",
        "        angle = float(tokens[0])\n",
        "        type = int(tokens[1])\n",
        "        if angle <= threshold and type == 0:\n",
        "            fp.append(i)\n",
        "        if angle > threshold and type == 1:\n",
        "            fn.append(i)\n",
        "\n",
        "    print('len(fp): ' + str(len(fp)))\n",
        "    print('len(fn): ' + str(len(fn)))\n",
        "\n",
        "    num_fp = len(fp)\n",
        "    num_fn = len(fn)\n",
        "\n",
        "    filename = 'data/lfw_test_pair.txt'\n",
        "    with open(filename, 'r') as file:\n",
        "        pair_lines = file.readlines()\n",
        "\n",
        "    for i in range(num_fp):\n",
        "        fp_id = fp[i]\n",
        "        fp_line = pair_lines[fp_id]\n",
        "        tokens = fp_line.split()\n",
        "        file0 = tokens[0]\n",
        "        copy_file(file0, '{}_fp_0.jpg'.format(i))\n",
        "        save_aligned(file0, '{}_fp_0_aligned.jpg'.format(i))\n",
        "        file1 = tokens[1]\n",
        "        copy_file(file1, '{}_fp_1.jpg'.format(i))\n",
        "        save_aligned(file1, '{}_fp_1_aligned.jpg'.format(i))\n",
        "\n",
        "    for i in range(num_fn):\n",
        "        fn_id = fn[i]\n",
        "        fn_line = pair_lines[fn_id]\n",
        "        tokens = fn_line.split()\n",
        "        file0 = tokens[0]\n",
        "        copy_file(file0, '{}_fn_0.jpg'.format(i))\n",
        "        save_aligned(file0, '{}_fn_0_aligned.jpg'.format(i))\n",
        "        file1 = tokens[1]\n",
        "        copy_file(file1, '{}_fn_1.jpg'.format(i))\n",
        "        save_aligned(file1, '{}_fn_1_aligned.jpg'.format(i))\n",
        "\n",
        "\n",
        "def save_aligned(old_fn, new_fn):\n",
        "    old_fn = os.path.join('data/lfw_funneled', old_fn)\n",
        "    _, landmarks = get_central_face_attributes(old_fn)\n",
        "    img = align_face(old_fn, landmarks)\n",
        "    new_fn = os.path.join('images', new_fn)\n",
        "    cv.imwrite(new_fn, img)\n",
        "\n",
        "\n",
        "def copy_file(old, new):\n",
        "    old_fn = os.path.join('data/lfw_funneled', old)\n",
        "    img = cv.imread(old_fn)\n",
        "    bboxes, landmarks = get_all_face_attributes(old_fn)\n",
        "    draw_bboxes(img, bboxes, landmarks)\n",
        "    cv.resize(img, (224, 224))\n",
        "    new_fn = os.path.join('images', new)\n",
        "    cv.imwrite(new_fn, img)\n",
        "\n",
        "\n",
        "def get_threshold():\n",
        "    with open(angles_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for line in lines:\n",
        "        tokens = line.split()\n",
        "        angle = float(tokens[0])\n",
        "        type = int(tokens[1])\n",
        "        data.append({'angle': angle, 'type': type})\n",
        "\n",
        "    min_error = 6000\n",
        "    min_threshold = 0\n",
        "\n",
        "    for d in data:\n",
        "        threshold = d['angle']\n",
        "        type1 = len([s for s in data if s['angle'] <= threshold and s['type'] == 0])\n",
        "        type2 = len([s for s in data if s['angle'] > threshold and s['type'] == 1])\n",
        "        num_errors = type1 + type2\n",
        "        if num_errors < min_error:\n",
        "            min_error = num_errors\n",
        "            min_threshold = threshold\n",
        "\n",
        "    # print(min_error, min_threshold)\n",
        "    return min_threshold\n",
        "\n",
        "\n",
        "def lfw_test(model):\n",
        "    filename = 'data/lfw-funneled.tgz'\n",
        "    if not os.path.isdir('data/lfw_funneled'):\n",
        "        print('Extracting {}...'.format(filename))\n",
        "        extract(filename)\n",
        "\n",
        "    # if not os.path.isfile(lfw_pickle):\n",
        "    print('Processing {}...'.format(lfw_pickle))\n",
        "    process()\n",
        "\n",
        "    # if not os.path.isfile(angles_file):\n",
        "    print('Evaluating {}...'.format(angles_file))\n",
        "    evaluate(model)\n",
        "\n",
        "    print('Calculating threshold...')\n",
        "    # threshold = 70.36\n",
        "    thres = get_threshold()\n",
        "    print('Calculating accuracy...')\n",
        "    acc = accuracy(thres)\n",
        "    print('Accuracy: {}%, threshold: {}'.format(acc * 100, thres))\n",
        "    return acc, thres\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    checkpoint = 'BEST_checkpoint.tar'\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    model = checkpoint['model'].module\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    acc, threshold = lfw_test(model)\n",
        "\n",
        "    print('Visualizing {}...'.format(angles_file))\n",
        "    visualize(threshold)\n",
        "\n",
        "    print('error analysis...')\n",
        "    error_analysis(threshold)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-72db468ffd34>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    def extract('/content/lfw-dataset.zip'):\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3U7sfY2ZlSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/utils.py\" ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIaAaULbZW3O",
        "colab_type": "code",
        "outputId": "5136a279-0121-4565-8134-8a3a63ed5edb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/DLDatasets\")\n",
        "!ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "align_faces.py\t detector.py\t   InsightFace-v2     __pycache__\n",
            "box_utils1.py\t first_stage.py    lfw_eval.py\t      py_cpu_nms.py\n",
            "box_utils.py\t focal_loss.ipynb  loader.py\t      retinaface.py\n",
            "config1.py\t image_aug.ipynb   models1.ipynb      train.py\n",
            "config.py\t image_aug.py\t   models.py\t      utils.py\n",
            "data\t\t _init_.py\t   msceleb.zip\t      visualization_utils.py\n",
            "data_augment.py  _init.py\t   multibox_loss1.py  wider_face.py\n",
            "data_gen.ipynb\t insightface\t   net.py\n",
            "data_gen.py\t insightface1\t   prior_box.py\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}