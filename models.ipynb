{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "models",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV_r3CfD8Y1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MTCNN\n",
        "#models.py\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: a float tensor with shape [batch_size, c, h, w].\n",
        "        Returns:\n",
        "            a float tensor with shape [batch_size, c*h*w].\n",
        "        \"\"\"\n",
        "\n",
        "        # without this pretrained model isn't working\n",
        "        x = x.transpose(3, 2).contiguous()\n",
        "\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class PNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(PNet, self).__init__()\n",
        "\n",
        "        # suppose we have input with size HxW, then\n",
        "        # after first layer: H - 2,\n",
        "        # after pool: ceil((H - 2)/2),\n",
        "        # after second conv: ceil((H - 2)/2) - 2,\n",
        "        # after last conv: ceil((H - 2)/2) - 4,\n",
        "        # and the same for W\n",
        "\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv1', nn.Conv2d(3, 10, 3, 1)),\n",
        "            ('prelu1', nn.PReLU(10)),\n",
        "            ('pool1', nn.MaxPool2d(2, 2, ceil_mode=True)),\n",
        "\n",
        "            ('conv2', nn.Conv2d(10, 16, 3, 1)),\n",
        "            ('prelu2', nn.PReLU(16)),\n",
        "\n",
        "            ('conv3', nn.Conv2d(16, 32, 3, 1)),\n",
        "            ('prelu3', nn.PReLU(32))\n",
        "        ]))\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(32, 2, 1, 1)\n",
        "        self.conv4_2 = nn.Conv2d(32, 4, 1, 1)\n",
        "\n",
        "        weights = np.load('mtcnn/weights/pnet.npy', allow_pickle=True)[()]\n",
        "        for n, p in self.named_parameters():\n",
        "            p.data = torch.FloatTensor(weights[n])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: a float tensor with shape [batch_size, 3, h, w].\n",
        "        Returns:\n",
        "            b: a float tensor with shape [batch_size, 4, h', w'].\n",
        "            a: a float tensor with shape [batch_size, 2, h', w'].\n",
        "        \"\"\"\n",
        "        x = self.features(x)\n",
        "        a = self.conv4_1(x)\n",
        "        b = self.conv4_2(x)\n",
        "        a = F.softmax(a, dim=1)\n",
        "        return b, a\n",
        "\n",
        "\n",
        "class RNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(RNet, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv1', nn.Conv2d(3, 28, 3, 1)),\n",
        "            ('prelu1', nn.PReLU(28)),\n",
        "            ('pool1', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
        "\n",
        "            ('conv2', nn.Conv2d(28, 48, 3, 1)),\n",
        "            ('prelu2', nn.PReLU(48)),\n",
        "            ('pool2', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
        "\n",
        "            ('conv3', nn.Conv2d(48, 64, 2, 1)),\n",
        "            ('prelu3', nn.PReLU(64)),\n",
        "\n",
        "            ('flatten', Flatten()),\n",
        "            ('conv4', nn.Linear(576, 128)),\n",
        "            ('prelu4', nn.PReLU(128))\n",
        "        ]))\n",
        "\n",
        "        self.conv5_1 = nn.Linear(128, 2)\n",
        "        self.conv5_2 = nn.Linear(128, 4)\n",
        "\n",
        "        weights = np.load('mtcnn/weights/rnet.npy', allow_pickle=True)[()]\n",
        "        for n, p in self.named_parameters():\n",
        "            p.data = torch.FloatTensor(weights[n])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: a float tensor with shape [batch_size, 3, h, w].\n",
        "        Returns:\n",
        "            b: a float tensor with shape [batch_size, 4].\n",
        "            a: a float tensor with shape [batch_size, 2].\n",
        "        \"\"\"\n",
        "        x = self.features(x)\n",
        "        a = self.conv5_1(x)\n",
        "        b = self.conv5_2(x)\n",
        "        a = F.softmax(a, dim=1)\n",
        "        return b, a\n",
        "\n",
        "\n",
        "class ONet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ONet, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv1', nn.Conv2d(3, 32, 3, 1)),\n",
        "            ('prelu1', nn.PReLU(32)),\n",
        "            ('pool1', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
        "\n",
        "            ('conv2', nn.Conv2d(32, 64, 3, 1)),\n",
        "            ('prelu2', nn.PReLU(64)),\n",
        "            ('pool2', nn.MaxPool2d(3, 2, ceil_mode=True)),\n",
        "\n",
        "            ('conv3', nn.Conv2d(64, 64, 3, 1)),\n",
        "            ('prelu3', nn.PReLU(64)),\n",
        "            ('pool3', nn.MaxPool2d(2, 2, ceil_mode=True)),\n",
        "\n",
        "            ('conv4', nn.Conv2d(64, 128, 2, 1)),\n",
        "            ('prelu4', nn.PReLU(128)),\n",
        "\n",
        "            ('flatten', Flatten()),\n",
        "            ('conv5', nn.Linear(1152, 256)),\n",
        "            ('drop5', nn.Dropout(0.25)),\n",
        "            ('prelu5', nn.PReLU(256)),\n",
        "        ]))\n",
        "\n",
        "        self.conv6_1 = nn.Linear(256, 2)\n",
        "        self.conv6_2 = nn.Linear(256, 4)\n",
        "        self.conv6_3 = nn.Linear(256, 10)\n",
        "\n",
        "        weights = np.load('mtcnn/weights/onet.npy', allow_pickle=True)[()]\n",
        "        for n, p in self.named_parameters():\n",
        "            p.data = torch.FloatTensor(weights[n])\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: a float tensor with shape [batch_size, 3, h, w].\n",
        "        Returns:\n",
        "            c: a float tensor with shape [batch_size, 10].\n",
        "            b: a float tensor with shape [batch_size, 4].\n",
        "            a: a float tensor with shape [batch_size, 2].\n",
        "        \"\"\"\n",
        "        x = self.features(x)\n",
        "        a = self.conv6_1(x)\n",
        "        b = self.conv6_2(x)\n",
        "        c = self.conv6_3(x)\n",
        "        a = F.softmax(a, dim=1)\n",
        "        return c, b, a"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}