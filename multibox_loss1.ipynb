{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multibox_loss1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLbg2d3ZKugU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#retinaface\n",
        "#layers\n",
        "#modules\n",
        "#multibox_loss\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from config1 import cfg_mnet\n",
        "from box_utils1 import match, log_sum_exp\n",
        "\n",
        "GPU = cfg_mnet['gpu_train']\n",
        "\n",
        "\n",
        "class MultiBoxLoss(nn.Module):\n",
        "    \"\"\"SSD Weighted Loss Function\n",
        "    Compute Targets:\n",
        "        1) Produce Confidence Target Indices by matching  ground truth boxes\n",
        "           with (default) 'priorboxes' that have jaccard index > threshold parameter\n",
        "           (default threshold: 0.5).\n",
        "        2) Produce localization target by 'encoding' variance into offsets of ground\n",
        "           truth boxes and their matched  'priorboxes'.\n",
        "        3) Hard negative mining to filter the excessive number of negative examples\n",
        "           that comes with using a large number of default bounding boxes.\n",
        "           (default negative:positive ratio 3:1)\n",
        "    Objective Loss:\n",
        "        L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
        "        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n",
        "        weighted by α which is set to 1 by cross val.\n",
        "        Args:\n",
        "            c: class confidences,\n",
        "            l: predicted boxes,\n",
        "            g: ground truth boxes\n",
        "            N: number of matched default boxes\n",
        "        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap,\n",
        "                 encode_target):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.threshold = overlap_thresh\n",
        "        self.background_label = bkg_label\n",
        "        self.encode_target = encode_target\n",
        "        self.use_prior_for_matching = prior_for_matching\n",
        "        self.do_neg_mining = neg_mining\n",
        "        self.negpos_ratio = neg_pos\n",
        "        self.neg_overlap = neg_overlap\n",
        "        self.variance = [0.1, 0.2]\n",
        "\n",
        "    def forward(self, predictions, priors, targets):\n",
        "        \"\"\"Multibox Loss\n",
        "        Args:\n",
        "            predictions (tuple): A tuple containing loc preds, conf preds,\n",
        "            and prior boxes from SSD net.\n",
        "                conf shape: torch.size(batch_size,num_priors,num_classes)\n",
        "                loc shape: torch.size(batch_size,num_priors,4)\n",
        "                priors shape: torch.size(num_priors,4)\n",
        "            ground_truth (tensor): Ground truth boxes and labels for a batch,\n",
        "                shape: [batch_size,num_objs,5] (last idx is the label).\n",
        "        \"\"\"\n",
        "\n",
        "        loc_data, conf_data, landm_data = predictions\n",
        "        priors = priors\n",
        "        num = loc_data.size(0)\n",
        "        num_priors = (priors.size(0))\n",
        "\n",
        "        # match priors (default boxes) and ground truth boxes\n",
        "        loc_t = torch.Tensor(num, num_priors, 4)\n",
        "        landm_t = torch.Tensor(num, num_priors, 10)\n",
        "        conf_t = torch.LongTensor(num, num_priors)\n",
        "        for idx in range(num):\n",
        "            truths = targets[idx][:, :4].data\n",
        "            labels = targets[idx][:, -1].data\n",
        "            landms = targets[idx][:, 4:14].data\n",
        "            defaults = priors.data\n",
        "            match(self.threshold, truths, defaults, self.variance, labels, landms, loc_t, conf_t, landm_t, idx)\n",
        "        if GPU:\n",
        "            loc_t = loc_t.cuda()\n",
        "            conf_t = conf_t.cuda()\n",
        "            landm_t = landm_t.cuda()\n",
        "\n",
        "        zeros = torch.tensor(0).cuda()\n",
        "        # landm Loss (Smooth L1)\n",
        "        # Shape: [batch,num_priors,10]\n",
        "        pos1 = conf_t > zeros\n",
        "        num_pos_landm = pos1.long().sum(1, keepdim=True)\n",
        "        N1 = max(num_pos_landm.data.sum().float(), 1)\n",
        "        pos_idx1 = pos1.unsqueeze(pos1.dim()).expand_as(landm_data)\n",
        "        landm_p = landm_data[pos_idx1].view(-1, 10)\n",
        "        landm_t = landm_t[pos_idx1].view(-1, 10)\n",
        "        loss_landm = F.smooth_l1_loss(landm_p, landm_t, reduction='sum')\n",
        "\n",
        "        pos = conf_t != zeros\n",
        "        conf_t[pos] = 1\n",
        "\n",
        "        # Localization Loss (Smooth L1)\n",
        "        # Shape: [batch,num_priors,4]\n",
        "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
        "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
        "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
        "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
        "\n",
        "        # Compute max conf across batch for hard negative mining\n",
        "        batch_conf = conf_data.view(-1, self.num_classes)\n",
        "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n",
        "\n",
        "        # Hard Negative Mining\n",
        "        loss_c[pos.view(-1, 1)] = 0  # filter out pos boxes for now\n",
        "        loss_c = loss_c.view(num, -1)\n",
        "        _, loss_idx = loss_c.sort(1, descending=True)\n",
        "        _, idx_rank = loss_idx.sort(1)\n",
        "        num_pos = pos.long().sum(1, keepdim=True)\n",
        "        num_neg = torch.clamp(self.negpos_ratio * num_pos, max=pos.size(1) - 1)\n",
        "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
        "\n",
        "        # Confidence Loss Including Positive and Negative Examples\n",
        "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
        "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
        "        conf_p = conf_data[(pos_idx + neg_idx).gt(0)].view(-1, self.num_classes)\n",
        "        targets_weighted = conf_t[(pos + neg).gt(0)]\n",
        "        loss_c = F.cross_entropy(conf_p, targets_weighted, reduction='sum')\n",
        "\n",
        "        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
        "        N = max(num_pos.data.sum().float(), 1)\n",
        "        loss_l /= N\n",
        "        loss_c /= N\n",
        "        loss_landm /= N1\n",
        "\n",
        "        return loss_l, loss_c, loss_landm"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}